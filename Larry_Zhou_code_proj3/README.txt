Almost all of the code I used was either referenced on piazza or in lecture. For the majority of part 1, I used laplacian and Gaussian filters/stacks to generate images. Everything implemented a Gaussian filter/stack of some sort, and it was just a matter of doing algebra to combine the images. For part 2, I followed the image_blending technique presented in lecture. In part 2.1, I created an A and b matrix based on the source image and calculated the gradients. I then inserted a pixel into the top left corner and the x component of Ax=b was just a reproduced image. The process was similar in 2.2, but the equations were more elaborate. Rather than looking at just 1 adjacent pixel, I looked at all of them. I also had to process each channel separately in order to preserve the colors of the image. I also manually created the masks for the image and resized them so that they were already the same size.
This project implemented pixel manipulation as well as laplacian and gaussian stacks. It was pretty interesting how we were able to blend images together using a mask to assist in the process. We started with unblurring images using a gaussian filter. This process was pretty straightforward. We took a gaussian filter, subtracted it from the original image, scaled it, then added it back to the original image. In Part 1.2, we created a hybrid image using the high and low frequencies of two images. When close up, the high frequency image is more visible. At a far away distance, the low frequency image becomes visible. In 1.3, we simply applied gaussian and lapalcian stacks to the previous concept. By doing so, we can break down an image into its two separate components. Finally, in Part 1.4, we combined everything in order to create a blended image using least squares. I used the orange apple example to debug, and created my own example using a sun and moon image. We essentially splined the images at the boundary so that there was a smooth transition between the two halves. For the test images of 1.4, I simply created a mask of black and white, making a mask of the same size as the orange and apple images. The left half was all white because I wanted to get the apple half from this side, while the right half was all black. My final image is in black and white because I did not have time to make it color, but making it color would be as simple as doing what I did in part 2.2. I would have had to process each channel separately and combine them at the end. And now e dive into part 2.2. On the test images, the picture of the hikers is the target, the penguin was the source, and the mask essentially captured the pixels from the penguin image that we wanted. Depending on how accurate the mask is, the image will blend more smoothly. this mask was pretty rough, but it was still able to produce decent images. This process involved using the algorithm described in class (image blending in 10 mins). I essentially expanded on part 2.1, created a more robust equation using all adjacent pixels rather than just 1, and processed the channnels separately in order to maintain color. I was also able to create a pretty amusing image with a penguin completely out of its environment by picking a different target image. The blending did not end that well because the mask isn't perfect and the difference in the penguin's skin tone vs the background is a little too much. As for my own image, I was able to create a pretty cool blend of a beach front and a space image. Again, the blending won't be as smooth if there a huge color discrepancy between where you are trying to blend the images. 
